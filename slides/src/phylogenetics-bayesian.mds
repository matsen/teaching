% Bayesian phylogenetics
% Erick Matsen

# Review: what does likelihood mean? Simulation interpretation?

#
Modern technology gives us the ability to *observe* in great detail
^imgw figures/inference/edge-detection-first.jpg
<aside class="notes">
Modern technology gives us the ability to *observe* in great detail, which is wonderful, but in order to actually gain understanding we need to simplify and abstract.
</aside>

&nbsp;


#
But very detailed observation is not the same as understanding
^imgw figures/inference/edge-detection.jpg

To *understand* we need to simplify and abstract.


# What abstractions do we have at our disposal?

# &nbsp;
<section data-background="figures/inference/abstraction-3_nofonts.svg"> </section>
&nbsp;

<aside class="notes">
* Here is the number 3, which is one abstraction we all know and love.
* This is such a basic abstraction that sometimes we forget that it's an abstraction at all.
* But it really is-- you can't have "three" in your pocket.
* You can have three coins, three pipette tips or three wombats in your pocket, but you can't have "three".
* I think we can all agree that conducting science without the letter 3 and its ilk would be rather difficult.

So now I'm going to skip over the square root of 2 and the completely bizarre notion of negative numbers to $x$.
</aside>


#
<section data-background="figures/inference/abstraction-x_nofonts.svg"> </section>

<aside class="notes">
* $x$ is like 3 in that it represents an abstract quantity, but in the case of $x$ it's an unknown abstract quantity.
* $x$ is SO COOL.
* $x$ is SO COOL because it allows us to describe knowledge in an implicit way.
</aside>


# $x$ is useful and we love it dearly!

&nbsp;

$x$ allows us to describe knowledge in an implicit way:

&nbsp;

$$
f(x)=y
$$

&nbsp;

then we can work towards *solving* for $x$.

&nbsp;

&nbsp;

<div class="fragment">
Alternatively, one might be interested in taking the average of $f(x)$ between two values $a$ and $b$.
</div>

<aside class="notes">
* For example, here we have $x$
* $x$ is SO COOL.
* It allows us to describe knowledge in an implicit way.
</aside>


# Define $\int_a^b f(x) \ dx$ as area
^imgh figures/inference/integral-auc.svg 550


# $1/(b-a) \cdot \int_a^b f(x) \ dx$ is average
^imgh figures/inference/integral-average.svg 550


# Variables allow us to solve
^imgw figures/inference/cannon-deterministic.svg

* *Problem 1*: given $y$, solve for $x$.
* *Problem 2*: predict if a 10% bigger charge will hit the castle. <br> Say the answer to this is $\text{hit}_{10}(x)$, such that $\text{hit}_{10}(x)$ is 1 if that $x$ will make the cannonball hit the castle, and 0 otherwise.

<aside class="notes">
* I will leave the actual solution as an exercise.
* But actually I would like to use this toy problem as a way to point out that normal sorts of variables only allow us to solve...
</aside>


# Variables allow us to solve
^imgw figures/inference/cannon-deterministic.svg

&nbsp;

&nbsp;

... in a *deterministic* framework.

<aside class="notes">
* This implicitly assumes that not only does the process happen in exactly the same way each time, it also means that we have a complete description of the process.
* Given x, the cannonball is going to take exactly the same trajectory each time.
* No variation in wind, no strange turbulence effects with slightly different shaped cannonballs, no anything that might make the path do anything other than strict Newtonian motion.
* So here is a picture of something that inhabits this world:
</aside>


#
<section data-background="figures/inference/coin-flipping-machine.jpg"> </section>

#
<section data-background="figures/inference/coin-flipping-hand.jpg"> </section>

&nbsp;


#
Life is a probabilistic process.

&nbsp;

&nbsp;

How do we abstract probabilistic quantities?

#
<section data-background="figures/inference/abstraction-X_nofonts.svg"> </section>

# Random variables $X$ abstract variables
It doesn't have a fixed value: *we have to "ask" it for a value.*

<img height="350" src="figures/inference/coin-flipping-hand.jpg">

Random variables are capricious, <br> but they are well defined behind their stochastic exterior.

<aside class="notes">
Random variables are defined according to their distribution.
</aside>

# Random variable sampling determined by *distributions*
&nbsp;

Sometimes discrete:

$$
\begin{align}
P(\text{heads}) & = 0.51 \\
P(\text{tails}) & = 0.49
\end{align}
$$

&nbsp;

<div class="fragment">
Sometimes continuous:

^imgw figures/inference/Gaussian_distribution.svg 500
</div>

# Working with *random variables* $X$:
&nbsp;

<div class="fragment">
We can solve for $X$ in "equations" like
$f(X) \sim Y$, obtaining expressions such as $\mathbb P(X \mid Y);$
this is called *inference*.
</div>

&nbsp;

&nbsp;

<div class="fragment">
We can also *average* with respect to $X$:

$$
\int f(X) \, d\mathbb P(X \mid Y)
$$

where now we are averaging out with respect to a probability.
</div>


# Probabilistic approach to prediction
^imgw figures/inference/cannon-probabilistic.svg

* $Y$: horizontal distance traveled by a cannonball (random variable)
* $X$: cannon angle (inferred random variable)
* *Problem 1*: given observed distribution $Y$, infer distribution of $X$.
* *Problem 2*: find probability that a 10% bigger charge will hit castle.

&nbsp;

1. Solve $f(X) = Y$ &nbsp; to get &nbsp; $\mathbb P(X \mid Y)$.
2. Integrate $\int \text{hit}_{10}(X) \, d\mathbb P(X \mid Y)$.


# Biological experiments are measurements with uncertainty
^imgw figures/inference/sequencing.svg


# Model-based statistical inference <span style="color:green"> ✓ </span>
&nbsp;

We can solve for $X$ in "equations" like $f(X) \sim Y$, <br>
*inferring* an unknown distribution for $X$ <br>
(what can we learn about the angle of the cannon).

&nbsp;

&nbsp;

<div class="fragment">
We can push uncertainty through an analysis using integrals like
$$
\int_a^b f(X) \, d\mathbb P(X \mid Y).
$$
(we don't care what the angle of the cannon is really, we just want to know with what probability the shot is going to hit the castle!)
</div>


# Bayes is [magic](http://en.wikipedia.org/wiki/Bayes%27_theorem)

&nbsp;

&nbsp;

&nbsp;

$$
P(X \mid D) \propto P(D \mid X) P(X)
$$


# Now, what is model-based statistical inference on *discrete mathematical objects*?

&nbsp;


# Motivation: we would like to decide whether an individual has been *superinfected*, i.e. infected with a second viral variant in a separate event

&nbsp;

^ imgw figures/inference/superinfection-definition.svg 550


# Integrate out phylogenetic uncertainty
^img figures/inference/phylogenetic-posterior.svg

&nbsp;

To decide superinfection, we would like to calculate
$$
\int_S f(X) \, d\mathbb P(X \mid Y)
$$
where $X$ is now a *phylogenetic-tree-valued random variable*.


# Time to count your blessings.

&nbsp;
<ul style="list-style-type: none;">
  <li class="fragment">
  *Real numbers are equipped with a total order.*
  &nbsp; &nbsp; &nbsp; &nbsp; ($3 < 4$)
  </li>
  <li class="fragment">
  *Real numbers are equipped with a simply-computed distance <br> that is compatible with the total order.*
  &nbsp; &nbsp; &nbsp; &nbsp; ($\,|7-3| = 4\,$)
  </li>
  <li class="fragment">
  *Real numbers form a continuum.*
  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ($2.9 < 2.95 < 3$)
  </li>
</ul>


# We can thus define the integral
^img figures/inference/riemann-integral.svg

for real-valued $\int_a^b f(x) dx$ and $\int_a^b f(X) \, d\mathbb P(X \mid Y)$.


# Integrating over phylogenetic trees?
Phylogenetic trees have discrete topologies, there is no canonical distance between them, nor a natural total order.

&nbsp;

<div class="fragment">
But we still want to do inference and integration in this setting!

^imgw figures/phylo/bayesian_phylo_hearts.svg 600
</div>


# Subtree-prune-regraft (rSPR) definition

&nbsp;

^imgw figures/phylo/spr-definition.svg

&nbsp;

These trees are then distance 1 apart.


# Tree graph connected by rSPR moves
^img figures/phylo/5-taxa-treespace-unrooted.svg


# [Metropolis-Hastings algorithm](http://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm)
* If you jump to a better tree, accept that move
* If you jump to a worse tree, accept that move with a non-zero probability
* It's all arranged so that you sample trees in proportion to their posterior probability

<br>

Try out [MCMC robot](https://phylogeny.uconn.edu/mcmc-robot/)!


# Markov chain Monte Carlo
^img figures/phylo/5-taxa-treespace-mcmc-unrooted.svg


# Subset to high probability nodes
^img figures/phylo/5-taxa-treespace-subgraph-unrooted.svg


# The top 4096 trees for a data set
^imgh figures/phylo/ds6.png


# The posterior probability of a tree is the probability that the observed tree is correct (given the model and priors)
* Bayesians sample from this posterior
* If you can deal with a prior, it's the statistically right thing to do
* Sometimes we aren't actually interested in the tree, so we can integrate it out
* But! Short alignment, 100 taxa = hours


